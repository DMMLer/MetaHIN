{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "import random\n",
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "random.seed(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = 'original/'\n",
    "output_dir = './'\n",
    "melu_output_dir = '../../../MeLU/movielens/'\n",
    "states = [ \"warm_up\", \"user_cold_testing\", \"item_cold_testing\", \"user_and_item_cold_testing\",\"meta_training\"]\n",
    "\n",
    "if not os.path.exists(\"{}/meta_training/\".format(output_dir)):\n",
    "    os.mkdir(\"{}/log/\".format(output_dir))\n",
    "    for state in states:\n",
    "        os.mkdir(\"{}/{}/\".format(output_dir, state))\n",
    "        os.mkdir(\"{}/{}/\".format(melu_output_dir, state))\n",
    "        if not os.path.exists(\"{}/{}/{}\".format(output_dir, \"log\", state)):\n",
    "            os.mkdir(\"{}/{}/{}\".format(output_dir, \"log\", state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000209"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ui_data = pd.read_csv(input_dir+'ratings.dat', names=['user', 'item', 'rating', 'timestamp'],sep=\"::\", engine='python')\n",
    "len(ui_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data = pd.read_csv(input_dir+'users.dat', names=['user', 'gender', 'age', 'occupation_code', 'zip'],\n",
    "                        sep=\"::\", engine='python')\n",
    "item_data = pd.read_csv(input_dir+'movies_extrainfos.dat', names=['item', 'title', 'year', 'rate', 'released', 'genre', 'director', 'writer', 'actors', 'plot', 'poster'],\n",
    "                        sep=\"::\", engine='python', encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_list = list(set(ui_data.user.tolist()) | set(user_data.user))\n",
    "item_list = list(set(ui_data.item.tolist()) | set(item_data.item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6040, 3881)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_num = len(user_list)\n",
    "item_num = len(item_list)\n",
    "user_num, item_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. user and item feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_list(fname):\n",
    "    list_ = []\n",
    "    with open(fname, encoding=\"utf-8\") as f:\n",
    "        for line in f.readlines():\n",
    "            list_.append(line.strip())\n",
    "    return list_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 25, 7978, 2186, 2, 7, 21, 3402)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rate_list = load_list(\"{}/m_rate.txt\".format(input_dir))\n",
    "genre_list = load_list(\"{}/m_genre.txt\".format(input_dir))\n",
    "actor_list = load_list(\"{}/m_actor.txt\".format(input_dir))\n",
    "director_list = load_list(\"{}/m_director.txt\".format(input_dir))\n",
    "gender_list = load_list(\"{}/m_gender.txt\".format(input_dir))\n",
    "age_list = load_list(\"{}/m_age.txt\".format(input_dir))\n",
    "occupation_list = load_list(\"{}/m_occupation.txt\".format(input_dir))\n",
    "zipcode_list = load_list(\"{}/m_zipcode.txt\".format(input_dir))\n",
    "len(rate_list), len(genre_list), len(actor_list), len(director_list), len(gender_list), len(age_list), len(occupation_list), len(zipcode_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def item_converting(row, rate_list, genre_list, director_list, actor_list):\n",
    "        rate_idx = torch.tensor([[rate_list.index(str(row['rate']))]]).long()\n",
    "        genre_idx = torch.zeros(1, 25).long()\n",
    "        for genre in str(row['genre']).split(\", \"):\n",
    "            idx = genre_list.index(genre)\n",
    "            genre_idx[0, idx] = 1  # one-hot vector\n",
    "        \n",
    "        director_idx = torch.zeros(1, 2186).long()\n",
    "        director_id = []\n",
    "        for director in str(row['director']).split(\", \"):\n",
    "            idx = director_list.index(re.sub(r'\\([^()]*\\)', '', director))\n",
    "            director_idx[0, idx] = 1\n",
    "            director_id.append(idx+1)  # id starts from 1, not index\n",
    "        actor_idx = torch.zeros(1, 8030).long()\n",
    "        actor_id = []\n",
    "        for actor in str(row['actors']).split(\", \"):\n",
    "            idx = actor_list.index(actor)\n",
    "            actor_idx[0, idx] = 1\n",
    "            actor_id.append(idx+1)\n",
    "        return torch.cat((rate_idx, genre_idx), 1), torch.cat((rate_idx, genre_idx, director_idx, actor_idx), 1), director_id, actor_id\n",
    "\n",
    "    def user_converting(row, gender_list, age_list, occupation_list, zipcode_list):\n",
    "        gender_idx = torch.tensor([[gender_list.index(str(row['gender']))]]).long()\n",
    "        age_idx = torch.tensor([[age_list.index(str(row['age']))]]).long()\n",
    "        occupation_idx = torch.tensor([[occupation_list.index(str(row['occupation_code']))]]).long()\n",
    "        zip_idx = torch.tensor([[zipcode_list.index(str(row['zip'])[:5])]]).long()\n",
    "        return torch.cat((gender_idx, age_idx, occupation_idx, zip_idx), 1)  # (1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hash map for item\n",
    "movie_fea_hete = {}\n",
    "movie_fea_homo = {}\n",
    "m_directors = {}\n",
    "m_actors = {}\n",
    "for idx, row in item_data.iterrows():\n",
    "    m_info = item_converting(row, rate_list, genre_list, director_list, actor_list)\n",
    "    movie_fea_hete[row['item']] = m_info[0]\n",
    "    movie_fea_homo[row['item']] = m_info[1]\n",
    "    m_directors[row['item']] = m_info[2]\n",
    "    m_actors[row['item']] = m_info[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hash map for user\n",
    "user_fea = {}\n",
    "for idx, row in user_data.iterrows():\n",
    "    u_info = user_converting(row, gender_list, age_list, occupation_list, zipcode_list)\n",
    "    user_fea[row['user']] = u_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. mp data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = [ \"warm_up\", \"user_cold_testing\", \"item_cold_testing\", \"user_and_item_cold_testing\",\"meta_training\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "    import collections\n",
    "    def reverse_dict(d):\n",
    "        # {1:[a,b,c], 2:[a,f,g],...}\n",
    "        re_d = collections.defaultdict(list)\n",
    "        for k, v_list in d.items():\n",
    "            for v in v_list:\n",
    "                re_d[v].append(k)\n",
    "        return dict(re_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7978, 2186)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_movies = reverse_dict(m_actors)\n",
    "d_movies = reverse_dict(m_directors)\n",
    "len(a_movies), len(d_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jsonKeys2int(x):\n",
    "    if isinstance(x, dict):\n",
    "            return {int(k):v for k,v in x.items()}\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2749it [00:00, 311524.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2749\n",
      "2749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2749"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = 'meta_training'\n",
    "\n",
    "support_u_movies = json.load(open(output_dir+state+'/support_u_movies.json','r'), object_hook=jsonKeys2int)\n",
    "query_u_movies= json.load(open(output_dir+state+'/query_u_movies.json','r'), object_hook=jsonKeys2int)\n",
    "support_u_movies_y = json.load(open(output_dir+state+'/support_u_movies_y.json','r'), object_hook=jsonKeys2int)\n",
    "query_u_movies_y = json.load(open(output_dir+state+'/query_u_movies_y.json','r'), object_hook=jsonKeys2int)\n",
    "if support_u_movies.keys() == query_u_movies.keys():\n",
    "    u_id_list = support_u_movies.keys()\n",
    "print(len(u_id_list))\n",
    "\n",
    "train_u_movies = {}\n",
    "if support_u_movies.keys() == query_u_movies.keys():\n",
    "    u_id_list = support_u_movies.keys()\n",
    "print(len(u_id_list))\n",
    "for idx, u_id in tqdm(enumerate(u_id_list)):\n",
    "    train_u_movies[int(u_id)] = []\n",
    "    train_u_movies[int(u_id)] += support_u_movies[u_id]+query_u_movies[u_id]\n",
    "len(train_u_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2749"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_u_id_list = list(u_id_list).copy()\n",
    "len(train_u_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/2749 [00:00<00:47, 57.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2749/2749 [01:20<00:00, 34.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2749 2749 2749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# get mp data \n",
    "print(state)\n",
    "\n",
    "u_m_u_movies = {}\n",
    "u_m_a_movies = {}\n",
    "u_m_d_movies = {}\n",
    "\n",
    "support_m_users = reverse_dict(support_u_movies)\n",
    "\n",
    "for u in tqdm(u_id_list):\n",
    "    u_m_u_movies[u] = {}\n",
    "    u_m_a_movies[u] = {}\n",
    "    u_m_d_movies[u] = {}\n",
    "    for m in support_u_movies[u]:\n",
    "        u_m_a_movies[u][m] = set([m])\n",
    "        for _a in m_actors[m]:\n",
    "            cur_ms = a_movies[_a]\n",
    "            u_m_a_movies[u][m].update(cur_ms)\n",
    "            \n",
    "        u_m_d_movies[u][m] = set([m])\n",
    "        for _d in m_directors[m]:\n",
    "            cur_ms = d_movies[_d]\n",
    "            u_m_d_movies[u][m].update(cur_ms)\n",
    "        \n",
    "        u_m_u_movies[u][m] = set([m])   # add itself to avoid empty tensor when build the support set\n",
    "        u_m_u_movies[u][m].update(support_u_movies[u].copy())\n",
    "        if m in support_m_users:  # for meta_training, only support set can be seen!!!\n",
    "            for _u in support_m_users[m]:  #  only include user in training set !!!!\n",
    "                cur_ms = support_u_movies[_u]  # list\n",
    "                u_m_u_movies[u][m].update(cur_ms)\n",
    "    \n",
    "    for m in query_u_movies[u]:\n",
    "        if m in u_m_a_movies[u] or m in u_m_d_movies[u] or m in u_m_u_movies[u]:\n",
    "            print('error!!!')\n",
    "            break\n",
    "            \n",
    "        u_m_a_movies[u][m] = set([m])\n",
    "        for _a in m_actors[m]:\n",
    "            cur_ms = a_movies[_a]\n",
    "            u_m_a_movies[u][m].update(cur_ms)\n",
    "            \n",
    "        u_m_d_movies[u][m] = set([m])\n",
    "        for _d in m_directors[m]:\n",
    "            cur_ms = d_movies[_d]\n",
    "            u_m_d_movies[u][m].update(cur_ms)\n",
    "        \n",
    "        u_m_u_movies[u][m] = set([m])   # add itself to avoid empty tensor when build the support set\n",
    "        u_m_u_movies[u][m].update(support_u_movies[u].copy())\n",
    "        if m in support_m_users:  # for meta_training, only support set can be seen!!!\n",
    "            for _u in support_m_users[m]:  #  only include user in training set !!!!\n",
    "                cur_ms = support_u_movies[_u]  # list\n",
    "                u_m_u_movies[u][m].update(cur_ms)\n",
    "        \n",
    "print(len(u_m_u_movies), len(u_m_a_movies), len(u_m_d_movies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2749it [04:12, 10.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, u_id in  tqdm(enumerate(u_id_list)):\n",
    "    support_x_app = None\n",
    "    support_um_app = []\n",
    "    support_umum_app = []\n",
    "    support_umam_app = []\n",
    "    support_umdm_app = []\n",
    "        \n",
    "    for m_id in support_u_movies[u_id]:\n",
    "        tmp_x_converted = torch.cat((movie_fea_hete[m_id], user_fea[u_id]), 1)\n",
    "        try:\n",
    "            support_x_app = torch.cat((support_x_app, tmp_x_converted), 0)\n",
    "        except:\n",
    "            support_x_app = tmp_x_converted\n",
    "\n",
    "        # meta-paths\n",
    "        # UM\n",
    "        support_um_app.append(torch.cat(list(map(lambda x: movie_fea_hete[x], support_u_movies[u_id])), dim=0))  # each element: (#neighbor, 26=1+25)\n",
    "        # UMUM\n",
    "        support_umum_app.append(torch.cat(list(map(lambda x: movie_fea_hete[x], u_m_u_movies[u_id][m_id])), dim=0))\n",
    "        # UMAM\n",
    "        support_umam_app.append(torch.cat(list(map(lambda x: movie_fea_hete[x], u_m_a_movies[u_id][m_id])), dim=0))\n",
    "        # UMDM\n",
    "        support_umdm_app.append(torch.cat(list(map(lambda x: movie_fea_hete[x], u_m_d_movies[u_id][m_id])), dim=0))\n",
    "        \n",
    "    support_y_app = torch.FloatTensor(support_u_movies_y[u_id])\n",
    "    \n",
    "    pickle.dump(support_x_app, open(\"{}/{}/support_x_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "    pickle.dump(support_y_app, open(\"{}/{}/support_y_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "    pickle.dump(support_um_app, open(\"{}/{}/support_um_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "    pickle.dump(support_umum_app, open(\"{}/{}/support_umum_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "    pickle.dump(support_umam_app, open(\"{}/{}/support_umam_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "    pickle.dump(support_umdm_app, open(\"{}/{}/support_umdm_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "   \n",
    "    query_x_app = None\n",
    "    query_um_app = []\n",
    "    query_umum_app = []\n",
    "    query_umam_app = []\n",
    "    query_umdm_app = []\n",
    "        \n",
    "    for m_id in query_u_movies[u_id]:\n",
    "        tmp_x_converted = torch.cat((movie_fea_hete[m_id], user_fea[u_id]), 1)\n",
    "        try:\n",
    "            query_x_app = torch.cat((query_x_app, tmp_x_converted), 0)\n",
    "        except:\n",
    "            query_x_app = tmp_x_converted\n",
    "\n",
    "        # meta-paths\n",
    "        # UM\n",
    "        query_um_app.append(torch.cat(list(map(lambda x: movie_fea_hete[x], support_u_movies[u_id])), dim=0))  # each element: (#neighbor, 26=1+25)\n",
    "        # UMUM\n",
    "        query_umum_app.append(torch.cat(list(map(lambda x: movie_fea_hete[x], u_m_u_movies[u_id][m_id])), dim=0))\n",
    "        # UMAM\n",
    "        query_umam_app.append(torch.cat(list(map(lambda x: movie_fea_hete[x], u_m_a_movies[u_id][m_id])), dim=0))\n",
    "        # UMDM\n",
    "        query_umdm_app.append(torch.cat(list(map(lambda x: movie_fea_hete[x], u_m_d_movies[u_id][m_id])), dim=0))\n",
    "        \n",
    "    query_y_app = torch.FloatTensor(query_u_movies_y[u_id])\n",
    "    \n",
    "    pickle.dump(query_x_app, open(\"{}/{}/query_x_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "    pickle.dump(query_y_app, open(\"{}/{}/query_y_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "    pickle.dump(query_um_app, open(\"{}/{}/query_um_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "    pickle.dump(query_umum_app,open(\"{}/{}/query_umum_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "    pickle.dump(query_umam_app,open(\"{}/{}/query_umam_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "    pickle.dump(query_umdm_app,open(\"{}/{}/query_umdm_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "   \n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "731it [00:00, 361092.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "731\n",
      "731\n",
      "3480 2749\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# state = 'warm_up'\n",
    "# state = 'user_cold_testing'\n",
    "# state = 'item_cold_testing'\n",
    "state = 'user_and_item_cold_testing'\n",
    "\n",
    "support_u_movies = json.load(open(output_dir+state+'/support_u_movies.json','r'), object_hook=jsonKeys2int)\n",
    "query_u_movies= json.load(open(output_dir+state+'/query_u_movies.json','r'), object_hook=jsonKeys2int)\n",
    "support_u_movies_y = json.load(open(output_dir+state+'/support_u_movies_y.json','r'), object_hook=jsonKeys2int)\n",
    "query_u_movies_y = json.load(open(output_dir+state+'/query_u_movies_y.json','r'), object_hook=jsonKeys2int)\n",
    "if support_u_movies.keys() == query_u_movies.keys():\n",
    "    u_id_list = support_u_movies.keys()\n",
    "print(len(u_id_list))\n",
    "\n",
    "cur_train_u_movies =  train_u_movies.copy()\n",
    "\n",
    "if support_u_movies.keys() == query_u_movies.keys():\n",
    "    u_id_list = support_u_movies.keys()\n",
    "print(len(u_id_list))\n",
    "for idx, u_id in tqdm(enumerate(u_id_list)):\n",
    "    if u_id not in cur_train_u_movies:\n",
    "        cur_train_u_movies[u_id] = []\n",
    "    cur_train_u_movies[u_id] += support_u_movies[u_id]\n",
    "\n",
    "print(len(cur_train_u_movies),  len(train_u_movies))\n",
    "print(len(set(train_u_id_list) & set(u_id_list)))\n",
    "\n",
    "(len(u_id_list) +  len(train_u_movies) - len(set(train_u_id_list) & set(u_id_list))) == len(set(cur_train_u_movies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 6/731 [00:00<00:15, 47.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_and_item_cold_testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 731/731 [00:24<00:00, 29.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "731 731 731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# get mp data \n",
    "print(state)\n",
    "\n",
    "u_m_u_movies = {}\n",
    "u_m_a_movies = {}\n",
    "u_m_d_movies = {}\n",
    "\n",
    "cur_train_m_users = reverse_dict(cur_train_u_movies)\n",
    "\n",
    "for u in tqdm(u_id_list):\n",
    "    u_m_u_movies[u] = {}\n",
    "    u_m_a_movies[u] = {}\n",
    "    u_m_d_movies[u] = {}\n",
    "    for m in support_u_movies[u]:\n",
    "        u_m_a_movies[u][m] = set([m])\n",
    "        for _a in m_actors[m]:\n",
    "            cur_ms = a_movies[_a]\n",
    "            u_m_a_movies[u][m].update(cur_ms)\n",
    "            \n",
    "        u_m_d_movies[u][m] = set([m])\n",
    "        for _d in m_directors[m]:\n",
    "            cur_ms = d_movies[_d]\n",
    "            u_m_d_movies[u][m].update(cur_ms)\n",
    "        \n",
    "        u_m_u_movies[u][m] = set([m])   # add itself to avoid empty tensor when build the support set\n",
    "        u_m_u_movies[u][m].update(cur_train_u_movies[u].copy())\n",
    "        if m in cur_train_m_users:  # for meta_training, only support set can be seen!!!\n",
    "            for _u in cur_train_m_users[m]:  #  only include user in training set !!!!\n",
    "                cur_ms = cur_train_u_movies[_u]  # list\n",
    "                u_m_u_movies[u][m].update(cur_ms)\n",
    "    \n",
    "    for m in query_u_movies[u]:\n",
    "        if m in u_m_a_movies[u] or m in u_m_d_movies[u] or m in u_m_u_movies[u]:\n",
    "            print('error!!!')\n",
    "            break\n",
    "            \n",
    "        u_m_a_movies[u][m] = set([m])\n",
    "        for _a in m_actors[m]:\n",
    "            cur_ms = a_movies[_a]\n",
    "            u_m_a_movies[u][m].update(cur_ms)\n",
    "            \n",
    "        u_m_d_movies[u][m] = set([m])\n",
    "        for _d in m_directors[m]:\n",
    "            cur_ms = d_movies[_d]\n",
    "            u_m_d_movies[u][m].update(cur_ms)\n",
    "        \n",
    "        u_m_u_movies[u][m] = set([m])   # add itself to avoid empty tensor when build the support set\n",
    "        u_m_u_movies[u][m].update(cur_train_u_movies[u].copy())\n",
    "        if m in cur_train_m_users:  # for meta_training, only support set can be seen!!!\n",
    "            for _u in cur_train_m_users[m]:  #  only include user in training set !!!!\n",
    "                cur_ms = cur_train_u_movies[_u]  # list\n",
    "                u_m_u_movies[u][m].update(cur_ms)\n",
    "        \n",
    "print(len(u_m_u_movies), len(u_m_a_movies), len(u_m_d_movies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "731it [01:21,  8.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, u_id in  tqdm(enumerate(u_id_list)):\n",
    "    support_x_app = None\n",
    "    support_um_app = []\n",
    "    support_umum_app = []\n",
    "    support_umam_app = []\n",
    "    support_umdm_app = []\n",
    "        \n",
    "    for m_id in support_u_movies[u_id]:\n",
    "        tmp_x_converted = torch.cat((movie_fea_hete[m_id], user_fea[u_id]), 1)\n",
    "        try:\n",
    "            support_x_app = torch.cat((support_x_app, tmp_x_converted), 0)\n",
    "        except:\n",
    "            support_x_app = tmp_x_converted\n",
    "\n",
    "        # meta-paths\n",
    "        # UM\n",
    "        support_um_app.append(torch.cat(list(map(lambda x: movie_fea_hete[x], cur_train_u_movies[u_id])), dim=0))  # each element: (#neighbor, 26=1+25)\n",
    "        # UMUM\n",
    "        support_umum_app.append(torch.cat(list(map(lambda x: movie_fea_hete[x], u_m_u_movies[u_id][m_id])), dim=0))\n",
    "        # UMAM\n",
    "        support_umam_app.append(torch.cat(list(map(lambda x: movie_fea_hete[x], u_m_a_movies[u_id][m_id])), dim=0))\n",
    "        # UMDM\n",
    "        support_umdm_app.append(torch.cat(list(map(lambda x: movie_fea_hete[x], u_m_d_movies[u_id][m_id])), dim=0))\n",
    "        \n",
    "    support_y_app = torch.FloatTensor(support_u_movies_y[u_id])\n",
    "    \n",
    "    pickle.dump(support_x_app, open(\"{}/{}/support_x_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "    pickle.dump(support_y_app, open(\"{}/{}/support_y_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "    pickle.dump(support_um_app, open(\"{}/{}/support_um_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "    pickle.dump(support_umum_app, open(\"{}/{}/support_umum_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "    pickle.dump(support_umam_app, open(\"{}/{}/support_umam_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "    pickle.dump(support_umdm_app, open(\"{}/{}/support_umdm_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "   \n",
    "    query_x_app = None\n",
    "    query_um_app = []\n",
    "    query_umum_app = []\n",
    "    query_umam_app = []\n",
    "    query_umdm_app = []\n",
    "        \n",
    "    for m_id in query_u_movies[u_id]:\n",
    "        tmp_x_converted = torch.cat((movie_fea_hete[m_id], user_fea[u_id]), 1)\n",
    "        try:\n",
    "            query_x_app = torch.cat((query_x_app, tmp_x_converted), 0)\n",
    "        except:\n",
    "            query_x_app = tmp_x_converted\n",
    "\n",
    "        # meta-paths\n",
    "        # UM\n",
    "        query_um_app.append(torch.cat(list(map(lambda x: movie_fea_hete[x], cur_train_u_movies[u_id]+[m_id])), dim=0))  # each element: (#neighbor, 26=1+25)\n",
    "        # UMUM\n",
    "        query_umum_app.append(torch.cat(list(map(lambda x: movie_fea_hete[x], u_m_u_movies[u_id][m_id])), dim=0))\n",
    "        # UMAM\n",
    "        query_umam_app.append(torch.cat(list(map(lambda x: movie_fea_hete[x], u_m_a_movies[u_id][m_id])), dim=0))\n",
    "        # UMDM\n",
    "        query_umdm_app.append(torch.cat(list(map(lambda x: movie_fea_hete[x], u_m_d_movies[u_id][m_id])), dim=0))\n",
    "        \n",
    "    query_y_app = torch.FloatTensor(query_u_movies_y[u_id])\n",
    "    \n",
    "    pickle.dump(query_x_app, open(\"{}/{}/query_x_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "    pickle.dump(query_y_app, open(\"{}/{}/query_y_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "    pickle.dump(query_um_app, open(\"{}/{}/query_um_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "    pickle.dump(query_umum_app,open(\"{}/{}/query_umum_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "    pickle.dump(query_umam_app,open(\"{}/{}/query_umam_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "    pickle.dump(query_umdm_app,open(\"{}/{}/query_umdm_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "   \n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(support_umum_app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2519, 26])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_umum_app[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2980, 3163)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_id, m_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
